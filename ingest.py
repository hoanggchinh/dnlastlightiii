import os
import time
from dotenv import load_dotenv
from langchain_community.document_loaders import (
    DirectoryLoader,
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone
from langchain_core.documents import Document

PINECONE_INDEX_NAME = "rag-do-an"
DATA_DIR = "data/"
EMBEDDING_MODEL = "models/text-embedding-004"
VECTOR_DIMENSION = 768


def load_documents_from_directory(directory):

    all_documents = [ ]

    print(f"üìÇ ƒêang qu√©t th∆∞ m·ª•c: {directory}")

    print("üìÑ Loading .txt files...")
    try:
        txt_loader = DirectoryLoader(
            directory,
            glob="**/*.txt",
            loader_cls=TextLoader,
            loader_kwargs={"encoding": "utf-8"},
            show_progress=True,
            use_multithreading=True
        )
        txt_docs = txt_loader.load()
        all_documents.extend(txt_docs)
        print(f"Loaded {len(txt_docs)} TXT files")
    except Exception as e:
        print(f"Error loading TXT: {e}")


    print("Loading .pdf files...")
    try:
        pdf_loader = DirectoryLoader(
            directory,
            glob="**/*.pdf",
            loader_cls=PyPDFLoader,
            show_progress=True,
            use_multithreading=True
        )
        pdf_docs = pdf_loader.load()
        all_documents.extend(pdf_docs)
        print(f"Loaded {len(pdf_docs)} PDF files")
    except Exception as e:
        print(f"Error loading PDF: {e}")

    print("Loading .docx files...")
    try:
        docx_loader = DirectoryLoader(
            directory,
            glob="**/*.docx",
            loader_cls=Docx2txtLoader,
            show_progress=True,
            use_multithreading=True
        )
        docx_docs = docx_loader.load()
        all_documents.extend(docx_docs)
        print(f"Loaded {len(docx_docs)} DOCX files")
    except Exception as e:
        print(f"Error loading DOCX: {e}")

    return all_documents


def smart_chunk_documents(documents):


    separators = [

        r"(?<=\n)‚îÅ{10,}",
        r"(?<=\n)={10,}",

        r"(?<=\n)#{1,3}\s+",
        r"(?<=\n)Ch∆∞∆°ng\s+[IVX0-9]+",
        r"(?<=\n)Ph·∫ßn\s+[IVX0-9]+",
        r"(?<=\n)ƒêi·ªÅu\s+\d+",

        r"(?<=\n)[0-9]+\.\s+[A-Z√Ä√Å·∫¢√É·∫†ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√Ç·∫¶·∫§·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜ]",
        "\n\n",
        r"(?<=\n)[-‚Ä¢‚óè‚óã]\s+",
        "\n",
        r"[.!?]\s+",
        " "
    ]

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2500,
        chunk_overlap=500,
        separators=separators,
        is_separator_regex=True,
        length_function=len,
    )

    chunks = text_splitter.split_documents(documents)

    cleaned_chunks = [ ]
    for chunk in chunks:
        content = chunk.page_content.strip()
        if len(content) > 50:
            cleaned_chunks.append(chunk)

    return cleaned_chunks


def enhance_chunk_with_context(chunks):

    enhanced_chunks = [ ]

    for i, chunk in enumerate(chunks):

        title = ""
        for line in chunk.page_content.split('\n') [ :3 ]:
            line_upper = line.strip()
            if len(line_upper) > 5 and (
                    line_upper.isupper() or
                    line.startswith('#') or
                    '‚îÅ' in line or
                    '=' in line
            ):
                title = line_upper.replace('#', '').replace('‚îÅ', '').replace('=', '').strip()
                break

        metadata = {
            "source": chunk.metadata.get("source", "unknown"),
            "chunk_id": i,
        }

        if title:
            metadata [ "title" ] = title

        if "page" in chunk.metadata:
            metadata [ "page" ] = chunk.metadata [ "page" ]

        if i > 0:
            prev_snippet = chunks [ i - 1 ].page_content [ -100: ].strip()
            metadata [ "previous_context" ] = prev_snippet

        enhanced_doc = Document(
            page_content=chunk.page_content,
            metadata=metadata
        )
        enhanced_chunks.append(enhanced_doc)

    return enhanced_chunks


def main():
    print("=" * 60)
    print("RAG INGESTION PIPELINE - TNUT CHATBOT (ENHANCED)")
    print("=" * 60)

    load_dotenv()
    google_api_key = os.getenv("GEMINI_API_KEY")
    pinecone_api_key = os.getenv("PINECONE_API_KEY")

    if not google_api_key or not pinecone_api_key:
        print("Missing API keys in .env file")
        return


    print(f"\nƒêang x√≥a d·ªØ li·ªáu c≈© trong index '{PINECONE_INDEX_NAME}'...")
    try:
        pc = Pinecone(api_key=pinecone_api_key)
        existing_indexes = pc.list_indexes().names()

        if PINECONE_INDEX_NAME not in existing_indexes:
            print(f"Index '{PINECONE_INDEX_NAME}' kh√¥ng t·ªìn t·∫°i!")
            return

        index = pc.Index(PINECONE_INDEX_NAME)
        index.delete(delete_all=True)
        print(f"ƒê√£ x√≥a to√†n b·ªô d·ªØ li·ªáu c≈©!")
        time.sleep(3)
    except Exception as e:
        print(f"L·ªói khi x√≥a d·ªØ li·ªáu: {e}")
        return

    documents = load_documents_from_directory(DATA_DIR)

    if not documents:
        print("Kh√¥ng t√¨m th·∫•y file n√†o!")
        return

    print(f"\nT·ªïng s·ªë documents: {len(documents)}")

    print("\n ƒêang chia nh·ªè documents b·∫±ng thu·∫≠t to√°n th√¥ng minh...")
    chunks = smart_chunk_documents(documents)
    print(f"ƒê√£ t·∫°o {len(chunks)} chunks")

    print("\n ƒêang th√™m context metadata...")
    enhanced_chunks = enhance_chunk_with_context(chunks)
    print(f"ƒê√£ enhance {len(enhanced_chunks)} chunks")

    if enhanced_chunks:
        print("\n" + "=" * 60)
        print("üìã PREVIEW 3 CHUNKS ƒê·∫¶U TI√äN:")
        print("=" * 60)
        for i in range(min(3, len(enhanced_chunks))):
            chunk = enhanced_chunks [ i ]
            print(f"\n--- CHUNK {i + 1} ---")
            print(f"Content: {chunk.page_content [ :200 ]}...")
            print(f"Metadata: {chunk.metadata}")
        print("=" * 60)

    print(f"\nKh·ªüi t·∫°o embedding model: {EMBEDDING_MODEL}...")
    embeddings = GoogleGenerativeAIEmbeddings(
        model=EMBEDDING_MODEL,
        google_api_key=google_api_key
    )

    print(f"\n K·∫øt n·ªëi t·ªõi Pinecone index: '{PINECONE_INDEX_NAME}'...")
    vectorstore = PineconeVectorStore(
        index_name=PINECONE_INDEX_NAME,
        embedding=embeddings
    )

    # UPLOAD
    print(f"\n ƒêang upload {len(enhanced_chunks)} chunks l√™n Pinecone...")
    print("=" * 60)

    batch_size = 10
    total_chunks = len(enhanced_chunks)
    total_batches = (total_chunks + batch_size - 1) // batch_size

    for i in range(0, total_chunks, batch_size):
        batch = enhanced_chunks [ i:i + batch_size ]
        batch_num = i // batch_size + 1

        print(f"Batch {batch_num}/{total_batches} (chunks {i + 1}-{min(i + batch_size, total_chunks)})...", end=" ")

        try:
            vectorstore.add_documents(batch)
            print("")
            time.sleep(2)
        except Exception as e:
            print(f"\n L·ªói t·∫°i batch {batch_num}: {e}")
            continue

    # SUMMARY
    print("\n" + "=" * 60)
    print(" HO√ÄN TH√ÄNH INGEST!")
    print("=" * 60)
    print(f" T·ªïng k·∫øt:")
    print(f"   ‚Ä¢ T·ªïng s·ªë documents: {len(documents)}")
    print(f"   ‚Ä¢ T·ªïng s·ªë chunks: {len(enhanced_chunks)}")
    print(f"   ‚Ä¢ Chunk size: 2500 chars")
    print(f"   ‚Ä¢ Chunk overlap: 500 chars")
    print(f"   ‚Ä¢ Index name: {PINECONE_INDEX_NAME}")
    print(f"   ‚Ä¢ Embedding model: {EMBEDDING_MODEL}")
    print("=" * 60)


if __name__ == "__main__":
    main()