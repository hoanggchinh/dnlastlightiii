import os
import time
from dotenv import load_dotenv
from langchain_community.document_loaders import (
    DirectoryLoader,
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

PINECONE_INDEX_NAME = "rag-do-an"
DATA_DIR = "data/"
EMBEDDING_MODEL = "models/text-embedding-004"
VECTOR_DIMENSION = 768


def load_documents_from_directory(directory):
    """
    Load t·∫•t c·∫£ file TXT, PDF, DOCX t·ª´ th∆∞ m·ª•c
    """
    all_documents = [ ]

    print(f"üìÇ ƒêang qu√©t th∆∞ m·ª•c: {directory}")

    # 1. Load TXT files
    print("üìÑ Loading .txt files...")
    try:
        txt_loader = DirectoryLoader(
            directory,
            glob="**/*.txt",
            loader_cls=TextLoader,
            loader_kwargs={"encoding": "utf-8"},
            show_progress=True,
            use_multithreading=True
        )
        txt_docs = txt_loader.load()
        all_documents.extend(txt_docs)
        print(f"‚úÖ Loaded {len(txt_docs)} TXT files")
    except Exception as e:
        print(f"‚ö†Ô∏è Error loading TXT: {e}")

    # 2. Load PDF files
    print("üìï Loading .pdf files...")
    try:
        pdf_loader = DirectoryLoader(
            directory,
            glob="**/*.pdf",
            loader_cls=PyPDFLoader,
            show_progress=True,
            use_multithreading=True
        )
        pdf_docs = pdf_loader.load()
        all_documents.extend(pdf_docs)
        print(f"‚úÖ Loaded {len(pdf_docs)} PDF files")
    except Exception as e:
        print(f"‚ö†Ô∏è Error loading PDF: {e}")

    # 3. Load DOCX files
    print("üìò Loading .docx files...")
    try:
        docx_loader = DirectoryLoader(
            directory,
            glob="**/*.docx",
            loader_cls=Docx2txtLoader,
            show_progress=True,
            use_multithreading=True
        )
        docx_docs = docx_loader.load()
        all_documents.extend(docx_docs)
        print(f"‚úÖ Loaded {len(docx_docs)} DOCX files")
    except Exception as e:
        print(f"‚ö†Ô∏è Error loading DOCX: {e}")

    return all_documents


def main():
    print("=" * 60)
    print("üöÄ RAG INGESTION PIPELINE - TNUT CHATBOT")
    print("=" * 60)

    # Load environment variables
    load_dotenv()
    google_api_key = os.getenv("GEMINI_API_KEY")
    pinecone_api_key = os.getenv("PINECONE_API_KEY")

    if not google_api_key or not pinecone_api_key:
        print("‚ùå Missing API keys in .env file")
        return

    # ============================================================
    # B∆Ø·ªöC 1: X√ìA TO√ÄN B·ªò D·ªÆ LI·ªÜU C≈® TRONG INDEX
    # ============================================================
    print(f"\nüóëÔ∏è  ƒêang x√≥a to√†n b·ªô d·ªØ li·ªáu c≈© trong index '{PINECONE_INDEX_NAME}'...")

    try:
        pc = Pinecone(api_key=pinecone_api_key)

        # Ki·ªÉm tra index c√≥ t·ªìn t·∫°i kh√¥ng
        existing_indexes = pc.list_indexes().names()

        if PINECONE_INDEX_NAME not in existing_indexes:
            print(f"‚ùå Index '{PINECONE_INDEX_NAME}' kh√¥ng t·ªìn t·∫°i!")
            print("Vui l√≤ng t·∫°o index tr∆∞·ªõc ho·∫∑c ki·ªÉm tra t√™n index.")
            return

        # X√≥a to√†n b·ªô vectors trong index
        index = pc.Index(PINECONE_INDEX_NAME)
        index.delete(delete_all=True)

        print(f"‚úÖ ƒê√£ x√≥a to√†n b·ªô d·ªØ li·ªáu c≈©!")
        time.sleep(3)  # Ch·ªù Pinecone x·ª≠ l√Ω xong

    except Exception as e:
        print(f"‚ùå L·ªói khi x√≥a d·ªØ li·ªáu: {e}")
        return

    # ============================================================
    # B∆Ø·ªöC 2: LOAD DOCUMENTS
    # ============================================================
    documents = load_documents_from_directory(DATA_DIR)

    if not documents:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file n√†o!")
        return

    print(f"\n‚úÖ T·ªïng s·ªë documents: {len(documents)}")

    # ============================================================
    # B∆Ø·ªöC 3: SPLIT TH√ÄNH CHUNKS
    # ============================================================
    print("\nüî™ ƒêang chia nh·ªè documents th√†nh chunks...")

    separators = [
        r"(?<=\n)Ch∆∞∆°ng\s+[IVX0-9]+",
        r"(?<=\n)Ph·∫ßn\s+[IVX0-9]+",
        r"(?<=\n)[IVX]+\.\s",
        r"(?<=\n)ƒêi·ªÅu\s+\d+",
        r"(?<=\n)[A-Z]\.\s",
        r"(?<=\n)\d+(\.\d+)+(\.)?\s",
        r"(?<=\n)\d+\.\s",
        r"(?<=\n)[a-z]\)\s",
        r"(?<=\n)-\s",
        "\n\n",
        "\n",
        " ",
        ""
    ]

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=separators,
        is_separator_regex=True
    )

    chunks = text_splitter.split_documents(documents)
    print(f"‚úÖ ƒê√£ t·∫°o {len(chunks)} chunks")

    # Preview chunk ƒë·∫ßu ti√™n
    if chunks:
        print("\n" + "=" * 60)
        print("üìã PREVIEW CHUNK ƒê·∫¶U TI√äN:")
        print("=" * 60)
        print(f"Content: {chunks [ 0 ].page_content [ :300 ]}...")
        print(f"\nMetadata: {chunks [ 0 ].metadata}")
        print("=" * 60)

    # ============================================================
    # B∆Ø·ªöC 4: T·∫†O EMBEDDINGS V√Ä UPLOAD L√äN PINECONE
    # ============================================================
    print(f"\nüîÆ Kh·ªüi t·∫°o embedding model: {EMBEDDING_MODEL}...")
    embeddings = GoogleGenerativeAIEmbeddings(
        model=EMBEDDING_MODEL,
        google_api_key=google_api_key
    )

    print(f"\nüìå K·∫øt n·ªëi t·ªõi Pinecone index: '{PINECONE_INDEX_NAME}'...")
    vectorstore = PineconeVectorStore(
        index_name=PINECONE_INDEX_NAME,
        embedding=embeddings
    )

    # Upload chunks theo batch
    print(f"\n‚¨ÜÔ∏è  ƒêang upload {len(chunks)} chunks l√™n Pinecone...")
    print("=" * 60)

    batch_size = 10
    total_chunks = len(chunks)
    total_batches = (total_chunks + batch_size - 1) // batch_size

    for i in range(0, total_chunks, batch_size):
        batch = chunks [ i:i + batch_size ]
        batch_num = i // batch_size + 1

        print(f"üì¶ Batch {batch_num}/{total_batches} (chunks {i + 1}-{min(i + batch_size, total_chunks)})...", end=" ")

        try:
            vectorstore.add_documents(batch)
            print("‚úÖ")
            time.sleep(2)  # Rate limiting ƒë·ªÉ tr√°nh b·ªã ch·∫∑n

        except Exception as e:
            print(f"\n‚ùå L·ªói t·∫°i batch {batch_num}: {e}")
            print("Ti·∫øp t·ª•c v·ªõi batch ti·∫øp theo...")
            continue

    # ============================================================
    # HO√ÄN TH√ÄNH
    # ============================================================
    print("\n" + "=" * 60)
    print("‚úÖ HO√ÄN TH√ÄNH INGESTION!")
    print("=" * 60)
    print(f"üìä T·ªïng k·∫øt:")
    print(f"   ‚Ä¢ T·ªïng s·ªë documents: {len(documents)}")
    print(f"   ‚Ä¢ T·ªïng s·ªë chunks: {len(chunks)}")
    print(f"   ‚Ä¢ Index name: {PINECONE_INDEX_NAME}")
    print(f"   ‚Ä¢ Embedding model: {EMBEDDING_MODEL}")
    print(f"   ‚Ä¢ D·ªØ li·ªáu c≈©: ƒê√É X√ìA")
    print(f"   ‚Ä¢ D·ªØ li·ªáu m·ªõi: ƒê√É UPLOAD")
    print("=" * 60)


if __name__ == "__main__":
    main()